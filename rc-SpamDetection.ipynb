{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Detection - NLP 662\n",
    "\n",
    "### by Cameron Wood and Tom el Safadi\n",
    "\n",
    "- This project is a spam detection model that leverages a bidirectional LSTM to output a binary classification for either 1 - spam, or 0 - not spam (called \"ham\"). The binary classification is a softmax layer but we use ReLU in order to handle the vanishing gradient problem.\n",
    "\n",
    "- We'll leverage the keras Tokenizer for handling our vocabulary and unique tokens internally. Then, we pad the tokens so that they are all the same length and normalized. This will be fed into an Embedding layer to vectorize our tokens before we finally begin training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import email\n",
    "import string\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional\n",
    "from keras.models import Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and unpack datasets\n",
    "\n",
    " - All of our datasets came from https://spamassassin.apache.org/old/publiccorpus/. Please run the below cell in order to download, unpack and move files to the appropriate directory before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-11-27 22:28:33--  https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham.tar.bz2\n",
      "Resolving spamassassin.apache.org (spamassassin.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
      "Connecting to spamassassin.apache.org (spamassassin.apache.org)|151.101.2.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1612216 (1.5M) [application/x-bzip2]\n",
      "Saving to: ‘20030228_easy_ham.tar.bz2’\n",
      "\n",
      "100%[======================================>] 1,612,216   --.-K/s   in 0.09s   \n",
      "\n",
      "2021-11-27 22:28:33 (17.2 MB/s) - ‘20030228_easy_ham.tar.bz2’ saved [1612216/1612216]\n",
      "\n",
      "--2021-11-27 22:28:33--  https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham_2.tar.bz2\n",
      "Resolving spamassassin.apache.org (spamassassin.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
      "Connecting to spamassassin.apache.org (spamassassin.apache.org)|151.101.2.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1077892 (1.0M) [application/x-bzip2]\n",
      "Saving to: ‘20030228_easy_ham_2.tar.bz2’\n",
      "\n",
      "100%[======================================>] 1,077,892   --.-K/s   in 0.08s   \n",
      "\n",
      "2021-11-27 22:28:34 (13.4 MB/s) - ‘20030228_easy_ham_2.tar.bz2’ saved [1077892/1077892]\n",
      "\n",
      "--2021-11-27 22:28:34--  https://spamassassin.apache.org/old/publiccorpus/20030228_spam.tar.bz2\n",
      "Resolving spamassassin.apache.org (spamassassin.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
      "Connecting to spamassassin.apache.org (spamassassin.apache.org)|151.101.2.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1183768 (1.1M) [application/x-bzip2]\n",
      "Saving to: ‘20030228_spam.tar.bz2’\n",
      "\n",
      "100%[======================================>] 1,183,768   --.-K/s   in 0.08s   \n",
      "\n",
      "2021-11-27 22:28:34 (14.7 MB/s) - ‘20030228_spam.tar.bz2’ saved [1183768/1183768]\n",
      "\n",
      "--2021-11-27 22:28:34--  https://spamassassin.apache.org/old/publiccorpus/20050311_spam_2.tar.bz2\n",
      "Resolving spamassassin.apache.org (spamassassin.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
      "Connecting to spamassassin.apache.org (spamassassin.apache.org)|151.101.2.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2059029 (2.0M) [application/x-bzip2]\n",
      "Saving to: ‘20050311_spam_2.tar.bz2’\n",
      "\n",
      "100%[======================================>] 2,059,029   --.-K/s   in 0.09s   \n",
      "\n",
      "2021-11-27 22:28:34 (21.3 MB/s) - ‘20050311_spam_2.tar.bz2’ saved [2059029/2059029]\n",
      "\n",
      "--2021-11-27 22:28:34--  https://spamassassin.apache.org/old/publiccorpus/20030228_hard_ham.tar.bz2\n",
      "Resolving spamassassin.apache.org (spamassassin.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
      "Connecting to spamassassin.apache.org (spamassassin.apache.org)|151.101.2.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1029898 (1006K) [application/x-bzip2]\n",
      "Saving to: ‘20030228_hard_ham.tar.bz2’\n",
      "\n",
      "100%[======================================>] 1,029,898   --.-K/s   in 0.07s   \n",
      "\n",
      "2021-11-27 22:28:35 (14.4 MB/s) - ‘20030228_hard_ham.tar.bz2’ saved [1029898/1029898]\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "!rm -rf spam_data\n",
    "!mkdir -p spam_data\n",
    "\n",
    "!wget https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham.tar.bz2\n",
    "!wget https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham_2.tar.bz2\n",
    "!wget https://spamassassin.apache.org/old/publiccorpus/20030228_spam.tar.bz2\n",
    "!wget https://spamassassin.apache.org/old/publiccorpus/20050311_spam_2.tar.bz2\n",
    "!wget https://spamassassin.apache.org/old/publiccorpus/20030228_hard_ham.tar.bz2\n",
    "    \n",
    "!tar xvjf 20030228_easy_ham_2.tar.bz2 > /dev/null  # I don't care about seeing this output\n",
    "!tar xvjf 20030228_easy_ham.tar.bz2 > /dev/null  # I don't care about seeing this output\n",
    "!tar xvjf 20030228_hard_ham.tar.bz2 > /dev/null  # I don't care about seeing this output\n",
    "!tar xvjf 20030228_spam.tar.bz2 > /dev/null  # I don't care about seeing this output\n",
    "!tar xvjf 20050311_spam_2.tar.bz2 > /dev/null  # I don't care about seeing this output\n",
    "\n",
    "!rm 20030228_easy_ham.tar.bz2\n",
    "!rm 20030228_easy_ham_2.tar.bz2\n",
    "!rm 20030228_hard_ham.tar.bz2\n",
    "!rm 20030228_spam.tar.bz2\n",
    "!rm 20050311_spam_2.tar.bz2\n",
    "\n",
    "!mv easy_ham spam_data\n",
    "!mv easy_ham_2 spam_data\n",
    "!mv hard_ham spam_data\n",
    "!mv spam spam_data\n",
    "!mv spam_2 spam_data\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to read and prepare data\n",
    "\n",
    "- `all_datasets_exist()` checks that the files are downloaded and unpacked. Run above cell if not.\n",
    "- `read_email()` reads the raw content of an email given its file path.\n",
    "- `get_email_content()` is a wrapper that uses read_email on all emails.\n",
    "- `remove_null()` is used to remove an email object that has no content. This is done after reading all the emails because we need to remove the given y label associated with it.\n",
    "- `prepare_datasets()` is the beginning of the pipeline before we start cleaning/tokenizing/padding. This function will leverage all of the functions mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/test datasets are loaded and ready!\n"
     ]
    }
   ],
   "source": [
    "def all_datasets_exist() -> bool:\n",
    "    \"\"\"\n",
    "\n",
    "    Function that checks for all the expected spam datasets to be unpacked.\n",
    "\n",
    "    Params:\n",
    "        - None\n",
    "\n",
    "    Returns:\n",
    "        - True or False\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    expected_dirs = [\"easy_ham\", \"easy_ham_2\", \"hard_ham\", \"spam\", \"spam_2\"]\n",
    "    folder = \"./spam_data/\"\n",
    "\n",
    "    for dir_name in expected_dirs:\n",
    "        path = folder + dir_name\n",
    "        if not os.path.exists(path):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def read_email(path):\n",
    "    file = open(path, encoding=\"latin1\")\n",
    "    try:\n",
    "        msg = email.message_from_file(file)\n",
    "        for part in msg.walk():\n",
    "            if part.get_content_type() == \"text/plain\":\n",
    "                return part.get_payload()  # raw text\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "\n",
    "def get_email_content(email_paths):\n",
    "    content = [read_email(path) for path in email_paths]\n",
    "    return content\n",
    "\n",
    "\n",
    "def remove_null(datasets, labels):\n",
    "    \"\"\"\n",
    "    Sometimes email content is empty when we receive it. We can't remove it as we\n",
    "    read the email becuase we have a matching y_label unpacked already and we need\n",
    "    to remove them at the same time.\n",
    "\n",
    "    Params:\n",
    "     - datasets (list): Raw emails to be evaluated.\n",
    "     - labels: (list): 1/0 y_label of the emails from dataset.\n",
    "    \n",
    "    Returns:\n",
    "     - (datasets, labels): removed of any null content\n",
    "    \"\"\"\n",
    "    not_null_idx = [i for i, o in enumerate(datasets) if o is not None]\n",
    "    return np.array(datasets)[not_null_idx], np.array(labels)[not_null_idx]\n",
    "\n",
    "\n",
    "def prepare_datasets():\n",
    "    \"\"\"\n",
    "    Start of the SpamDetection pipeline which involves actually unpacking and splitting\n",
    "    the datasets into train and test data. You'll need to make sure to unpack the tar files\n",
    "    inside the spam_data directory (see the function `all_datasets_exist` for more detail).\n",
    "\n",
    "    Params:\n",
    "     - None\n",
    "\n",
    "     Returns:\n",
    "     - 4-tuple of train/test datasets with y labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not all_datasets_exist():\n",
    "        print(\"Please run the above cell.\")\n",
    "    print(\"Training/test datasets are loaded and ready!\")\n",
    "\n",
    "    # gather all file names unpacked\n",
    "    path = \"./spam_data/\"\n",
    "    ham_files = [\n",
    "        glob.glob(path + \"easy_ham/*\"),\n",
    "        glob.glob(path + \"easy_ham_2/*\"),\n",
    "        glob.glob(path + \"hard_ham/*\"),\n",
    "    ]\n",
    "    spam_files = [glob.glob(path + \"spam/*\"), glob.glob(path + \"spam_2/*\")]\n",
    "\n",
    "    ham_sample = np.array([train_test_split(o) for o in ham_files], dtype=object)\n",
    "\n",
    "    ham_train = np.array([])\n",
    "    ham_test = np.array([])\n",
    "    for o in ham_sample:\n",
    "        ham_train = np.concatenate((ham_train, o[0]), axis=0)\n",
    "        ham_test = np.concatenate((ham_test, o[1]), axis=0)\n",
    "\n",
    "    spam_sample = np.array([train_test_split(o) for o in spam_files], dtype=object)\n",
    "\n",
    "    spam_train = np.array([])\n",
    "    spam_test = np.array([])\n",
    "    for o in spam_sample:\n",
    "        spam_train = np.concatenate((spam_train, o[0]), axis=0)\n",
    "        spam_test = np.concatenate((spam_test, o[1]), axis=0)\n",
    "\n",
    "    # attach labels to data (0 - ham, 1 - spam)\n",
    "    ham_train_label = [0] * ham_train.shape[0]  # type: ignore\n",
    "    spam_train_label = [1] * spam_train.shape[0]  # type: ignore\n",
    "    ham_test_label = [0] * ham_test.shape[0]  # type: ignore\n",
    "    spam_test_label = [1] * spam_test.shape[0]  # type: ignore\n",
    "\n",
    "    x_test = np.concatenate((ham_test, spam_test))\n",
    "    y_test = np.concatenate((ham_test_label, spam_test_label))\n",
    "    x_train = np.concatenate((ham_train, spam_train))\n",
    "    y_train = np.concatenate((ham_train_label, spam_train_label))\n",
    "\n",
    "    # shuffle\n",
    "    train_shuffle_index = np.random.permutation(np.arange(0, x_train.shape[0]))  # type: ignore\n",
    "    test_shuffle_index = np.random.permutation(np.arange(0, x_test.shape[0]))  # type:ignore\n",
    "\n",
    "    x_train = x_train[train_shuffle_index]\n",
    "    y_train = y_train[train_shuffle_index]\n",
    "\n",
    "    x_test = x_test[test_shuffle_index]\n",
    "    y_test = y_test[test_shuffle_index]\n",
    "\n",
    "    # email content for training\n",
    "    x_train = get_email_content(x_train)\n",
    "    x_test = get_email_content(x_test)\n",
    "\n",
    "    x_train, y_train = remove_null(x_train, y_train)\n",
    "    x_test, y_test = remove_null(x_test, y_test)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "x_train, y_train, x_test, y_test = prepare_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning, Tokenizing and Padding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_clean(data):\n",
    "    \"\"\"\n",
    "    Do some preprocess cleaning with the data from emails. We want to do some\n",
    "    normalization so there is no bias for certain anomalies within the data.\n",
    "\n",
    "    Params:\n",
    "     - data (str): The raw content of an email\n",
    "\n",
    "    Returns:\n",
    "     - data (str): normalized and clean data\n",
    "    \"\"\"\n",
    "\n",
    "    data = data.replace(\"\\n\", \"\").lower().strip()\n",
    "    data = re.sub(r\"http\\S+\", \"\", data)  # no hyperlink\n",
    "    data = re.sub(r'\\d+', '', data) # no numbers\n",
    "    data = data.translate(str.maketrans(dict.fromkeys(string.punctuation)))\n",
    "    return data\n",
    "\n",
    "\n",
    "def tokenize_and_pad(x_train, x_test):\n",
    "    max_feature = 50000  # how many unique words\n",
    "    max_len = 2000 # max number of words\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=max_feature)\n",
    "\n",
    "    tokenizer.fit_on_texts(x_train)\n",
    "    x_train_features = np.array(tokenizer.texts_to_sequences(x_train), dtype=object)\n",
    "    x_test_features = np.array(tokenizer.texts_to_sequences(x_test), dtype=object)\n",
    "\n",
    "    x_train_features = pad_sequences(x_train_features,maxlen=max_len)\n",
    "    x_test_features = pad_sequences(x_test_features,maxlen=max_len)\n",
    "\n",
    "    return x_train_features, x_test_features\n",
    "\n",
    "\n",
    "\n",
    "x_train = [preprocess_clean(o) for o in x_train]\n",
    "x_test = [preprocess_clean(o) for o in x_test]\n",
    "\n",
    "x_train_features, x_test_features = tokenize_and_pad(x_train, x_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model\n",
    " - This is a bidirectional LSTM to output a binary classification for either 1 - spam, or 0 - not spam (called \"ham\"). The binary classification is a softmax layer but we use ReLU in order to handle the vanishing gradient problem. We'll use adam optimizer for our gradient descent and cross entropy as our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "   \n",
    "    max_feature = 50000  # how many unique words\n",
    "    max_len = 2000 # max number of words\n",
    "    embedding_vecor_length = 32\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Embedding(max_feature, embedding_vecor_length, input_length=max_len))\n",
    "    model.add(Bidirectional(LSTM(64)))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # print(model.summary())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "8/8 [==============================] - 59s 7s/step - loss: 0.6909 - accuracy: 0.7130 - val_loss: 0.6852 - val_accuracy: 0.7824\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 54s 7s/step - loss: 0.6765 - accuracy: 0.7981 - val_loss: 0.6580 - val_accuracy: 0.7824\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 55s 7s/step - loss: 0.5971 - accuracy: 0.7981 - val_loss: 0.5216 - val_accuracy: 0.7824\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 55s 7s/step - loss: 0.5102 - accuracy: 0.7981 - val_loss: 0.5246 - val_accuracy: 0.7824\n",
      "Epoch 5/20\n",
      "8/8 [==============================] - 54s 7s/step - loss: 0.5040 - accuracy: 0.7981 - val_loss: 0.5179 - val_accuracy: 0.7824\n",
      "Epoch 6/20\n",
      "8/8 [==============================] - 54s 7s/step - loss: 0.4981 - accuracy: 0.7981 - val_loss: 0.5110 - val_accuracy: 0.7824\n",
      "Epoch 7/20\n",
      "8/8 [==============================] - 54s 7s/step - loss: 0.4906 - accuracy: 0.7981 - val_loss: 0.4935 - val_accuracy: 0.7824\n",
      "Epoch 8/20\n",
      "8/8 [==============================] - 53s 7s/step - loss: 0.4639 - accuracy: 0.7981 - val_loss: 0.4549 - val_accuracy: 0.7824\n",
      "Epoch 9/20\n",
      "8/8 [==============================] - 54s 7s/step - loss: 0.4020 - accuracy: 0.7981 - val_loss: 0.3724 - val_accuracy: 0.7824\n",
      "Epoch 10/20\n",
      "8/8 [==============================] - 54s 7s/step - loss: 0.3073 - accuracy: 0.7989 - val_loss: 0.2651 - val_accuracy: 0.8089\n",
      "Epoch 11/20\n",
      "8/8 [==============================] - 53s 7s/step - loss: 0.2205 - accuracy: 0.9006 - val_loss: 0.2040 - val_accuracy: 0.9431\n",
      "Epoch 12/20\n",
      "8/8 [==============================] - 52s 6s/step - loss: 0.1693 - accuracy: 0.9665 - val_loss: 0.1706 - val_accuracy: 0.9633\n",
      "Epoch 13/20\n",
      "8/8 [==============================] - 53s 7s/step - loss: 0.1472 - accuracy: 0.9787 - val_loss: 0.1450 - val_accuracy: 0.9672\n",
      "Epoch 14/20\n",
      "8/8 [==============================] - 53s 7s/step - loss: 0.1081 - accuracy: 0.9823 - val_loss: 0.1186 - val_accuracy: 0.9688\n",
      "Epoch 15/20\n",
      "8/8 [==============================] - 53s 7s/step - loss: 0.0779 - accuracy: 0.9889 - val_loss: 0.1074 - val_accuracy: 0.9727\n",
      "Epoch 16/20\n",
      "8/8 [==============================] - 53s 7s/step - loss: 0.0505 - accuracy: 0.9905 - val_loss: 0.1285 - val_accuracy: 0.9672\n",
      "Epoch 17/20\n",
      "8/8 [==============================] - 53s 7s/step - loss: 0.0429 - accuracy: 0.9900 - val_loss: 0.0946 - val_accuracy: 0.9587\n",
      "Epoch 18/20\n",
      "8/8 [==============================] - 52s 6s/step - loss: 0.0333 - accuracy: 0.9939 - val_loss: 0.0947 - val_accuracy: 0.9735\n",
      "Epoch 19/20\n",
      "8/8 [==============================] - 51s 6s/step - loss: 0.0280 - accuracy: 0.9942 - val_loss: 0.0748 - val_accuracy: 0.9766\n",
      "Epoch 20/20\n",
      "8/8 [==============================] - 52s 6s/step - loss: 0.0259 - accuracy: 0.9945 - val_loss: 0.1134 - val_accuracy: 0.9719\n"
     ]
    }
   ],
   "source": [
    "def train_model(model):\n",
    "\n",
    "    history = model.fit(\n",
    "        x_train_features,\n",
    "        y_train,\n",
    "        batch_size=512,\n",
    "        epochs=20,\n",
    "        validation_data=(x_test_features, y_test)\n",
    "    )\n",
    "    print(\"Finished training!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp2021]",
   "language": "python",
   "name": "conda-env-nlp2021-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
